{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "from sklearn.model_selection import train_test_split\n",
    "disable_eager_execution()\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/tesis/scisumm/scisumm.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['summary']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing out train and test sets\n",
    "# print('X_train : ')\n",
    "# print(X_train.head())\n",
    "# print(X_train.shape)\n",
    "# print('')\n",
    "# print('X_test : ')\n",
    "# print(X_test.head())\n",
    "# print(X_test.shape)\n",
    "# print('')\n",
    "# print('y_train : ')\n",
    "# print(y_train.head())\n",
    "# print(y_train.shape)\n",
    "# print('')\n",
    "# print('y_test : ')\n",
    "# print(y_test.head())\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.str.lower()\n",
    "X_test  = X_test.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "# X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "train   = [tokenizer.tokenize(s.lower()) for s in X_train]\n",
    "test    = [tokenizer.tokenize(s.lower()) for s in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stopword Removal\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "listStopword = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus kata-kata di stopwords\n",
    "train_stopwords = []\n",
    "for sent in train:\n",
    "  filter = [s for s in sent if s not in listStopword]\n",
    "  train_stopwords.append(filter)\n",
    "# train_stopwords\n",
    "\n",
    "test_stopwords = []\n",
    "for sent in test:\n",
    "  filter = [s for s in sent if s not in listStopword]\n",
    "  test_stopwords.append(filter)\n",
    "# test_stopwords\n",
    "\n",
    "# output merupakan sisa kata yg sudah dihapus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train   = [' '.join(t) for t in train_stopwords]\n",
    "test    = [' '.join(t) for t in test_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "train   = [ps.stem(w) for w in train]\n",
    "test    = [ps.stem(w) for w in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "train   = vec.fit_transform(train)\n",
    "test    = vec.transform(test)\n",
    "\n",
    "train   = train.toarray()\n",
    "test    = test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756, 60704)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253, 60704)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.01000668, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.00778281, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.00396712, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_3644\\1079192181.py:68: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=define_model, verbose=1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 239, in fit\n    y = np.array(y)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 280. MiB for an array with shape (604, 60704) and data type float64\n\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 239, in fit\n    y = np.array(y)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 280. MiB for an array with shape (605, 60704) and data type float64\n\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 248, in fit\n    return super().fit(x, y, **kwargs)\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 164, in fit\n    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n  File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_3644\\1079192181.py\", line 19, in define_model\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 213, in assert_input_compatibility\n    raise TypeError(\nTypeError: Inputs to a layer should be tensors. Got 'None' (of type <class 'NoneType'>) as input for layer 'dense'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 93\u001b[0m\n\u001b[0;32m     84\u001b[0m param_grid \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m     85\u001b[0m     start_dim\u001b[39m=\u001b[39mstart_dim,\n\u001b[0;32m     86\u001b[0m     intermediate_dim\u001b[39m=\u001b[39mintermediate_dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m     epochs \u001b[39m=\u001b[39m epochs\n\u001b[0;32m     90\u001b[0m )\n\u001b[0;32m     92\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mmodel, param_grid\u001b[39m=\u001b[39mparam_grid, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m grid_result \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39;49mfit(train, train)\n\u001b[0;32m     95\u001b[0m \u001b[39m# summarize results\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest: \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m using \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (grid_result\u001b[39m.\u001b[39mbest_score_, grid_result\u001b[39m.\u001b[39mbest_params_))\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[0;32m    845\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[0;32m    849\u001b[0m     )\n\u001b[1;32m--> 851\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[0;32m    853\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 239, in fit\n    y = np.array(y)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 280. MiB for an array with shape (604, 60704) and data type float64\n\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 239, in fit\n    y = np.array(y)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 280. MiB for an array with shape (605, 60704) and data type float64\n\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 248, in fit\n    return super().fit(x, y, **kwargs)\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 164, in fit\n    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n  File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_3644\\1079192181.py\", line 19, in define_model\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 213, in assert_input_compatibility\n    raise TypeError(\nTypeError: Inputs to a layer should be tensors. Got 'None' (of type <class 'NoneType'>) as input for layer 'dense'.\n"
     ]
    }
   ],
   "source": [
    "# === HYPERMETER TUNING ===\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras import Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setup the network parameters:\n",
    "original_dim = train.shape[1]\n",
    "input_shape = (original_dim, )\n",
    "latent_dim = 0\n",
    "\n",
    "# build encoder model\n",
    "def define_model(start_dim=0, intermediate_dim=0, latent_dim=0):\n",
    "    model = Sequential()\n",
    "    inputs      = model.add(Input(shape=input_shape, name='encoder_input'))\n",
    "    x           = model.add(Dense(start_dim, activation='relu')(inputs))\n",
    "    x           = model.add(Dense(intermediate_dim, activation='relu')(x))\n",
    "    z_mean      = model.add(Dense(latent_dim, name='z_mean')(x))\n",
    "    z_log_var   = model.add(Dense(latent_dim, name='z_log_var')(x))\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch   = K.shape(z_mean)[0]\n",
    "        dim     = K.int_shape(z_mean)[1]\n",
    "        # by default, random_normal has mean = 0 and std = 1.0\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "    # Instantiate the encoder model:\n",
    "    encoder = Model(inputs, z_mean, name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    # Build the decoder model:\n",
    "    # def define_deccoder(start_dim=0, intermediate_dim=0, latent_dim=0):\n",
    "    model = Sequential()\n",
    "    latent_inputs   = model.add(Input(shape=(latent_dim,), name='z_sampling'))\n",
    "    x               = model.add(Dense(intermediate_dim, activation='relu')(latent_inputs))\n",
    "    x               = model.add(Dense(start_dim, activation='relu')(x))\n",
    "    outputs         = model.add(Dense(original_dim, activation='sigmoid')(x))\n",
    "\n",
    "    # Instantiate the decoder model:\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    decoder.summary()\n",
    "\n",
    "    # Instantiate the VAE model:\n",
    "    outputs = decoder(encoder(inputs))\n",
    "    vae     = Model(inputs, outputs, name='vae_mlp')\n",
    "    vae.summary()\n",
    "\n",
    "    # As in the Keras tutorial, we define a custom loss function:\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        xent_loss   = loss_object(x, x_decoded_mean)\n",
    "        kl_loss     = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "    # We compile the model:\n",
    "    vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "    return vae\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(build_fn=define_model, verbose=1)\n",
    "\n",
    "# model = vae(build_fn=define_model, verbose=1)\n",
    "\n",
    "# start_dim           = [200, 250, 500, 1000]\n",
    "# intermediate_dim    = [100, 125, 150, 500]\n",
    "# latent_dim          = [2,4,8,10,12,14,16,18,20]\n",
    "# batch_size          = [100, 150, 200, 250]\n",
    "# epochs              = [10,20,30,40,50]\n",
    "\n",
    "start_dim           = [200]\n",
    "intermediate_dim    = [100]\n",
    "latent_dim          = [2]\n",
    "batch_size          = [100]\n",
    "epochs              = [10]\n",
    "\n",
    "param_grid = dict(\n",
    "    start_dim=start_dim,\n",
    "    intermediate_dim=intermediate_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    batch_size=batch_size,\n",
    "    epochs = epochs\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n",
    "grid_result = grid.fit(train, train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean = %f (std=%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "\n",
    "# Finally, we train the model:\n",
    "\n",
    "results = model.fit(train, train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(test, test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 72593)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              72594000  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " z_mean (Dense)              (None, 250)               125250    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 73,219,750\n",
      "Trainable params: 73,219,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras import backend as K\n",
    "from keras import Model\n",
    "\n",
    "# Setup the network parameters:\n",
    "original_dim = (72593)\n",
    "input_shape = (original_dim, )\n",
    "# intermediate_dim = 200\n",
    "batch_size  = 16\n",
    "latent_dim  = 250\n",
    "epochs      = 10\n",
    "\n",
    "# Map inputs to the latent distribution parameters:\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs      = Input(shape=input_shape, name='encoder_input')\n",
    "# x           = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "x           = Dense(1000, activation='relu')(inputs)\n",
    "x           = Dense(500, activation='relu')(x)\n",
    "z_mean      = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var   = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# Use those parameters to sample new points from the latent space:\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "# z = z_mean + sqrt(var) * epsilon\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch   = K.shape(z_mean)[0]\n",
    "    dim     = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# Instantiate the encoder model:\n",
    "encoder = Model(inputs, z_mean, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 250)]             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 500)               125500    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1000)              501000    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 60704)             60764704  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,391,204\n",
      "Trainable params: 61,391,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the decoder model:\n",
    "latent_inputs   = Input(shape=(latent_dim,), name='z_sampling')\n",
    "# x               = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "x               = Dense(500, activation='relu')(latent_inputs)\n",
    "x               = Dense(1000, activation='relu')(x)\n",
    "outputs         = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# Instantiate the decoder model:\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 60704)]           0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 250)               61330750  \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 60704)             61391204  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,721,954\n",
      "Trainable params: 122,721,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the VAE model:\n",
    "outputs = decoder(encoder(inputs))\n",
    "vae     = Model(inputs, outputs, name='vae_mlp')\n",
    "vae.summary()\n",
    "\n",
    "# As in the Keras tutorial, we define a custom loss function:\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss   = loss_object(x, x_decoded_mean)\n",
    "    kl_loss     = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 756 samples, validate on 253 samples\n",
      "Epoch 1/10\n",
      "756/756 [==============================] - ETA: 0s - loss: 0.0761"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 83s 110ms/sample - loss: 0.0761 - val_loss: 0.0036\n",
      "Epoch 2/10\n",
      "756/756 [==============================] - 67s 88ms/sample - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 3/10\n",
      "756/756 [==============================] - 65s 87ms/sample - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 4/10\n",
      "756/756 [==============================] - 70s 93ms/sample - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 5/10\n",
      "756/756 [==============================] - 69s 92ms/sample - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 6/10\n",
      "756/756 [==============================] - 64s 84ms/sample - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 7/10\n",
      "756/756 [==============================] - 63s 84ms/sample - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 8/10\n",
      "756/756 [==============================] - 62s 83ms/sample - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 9/10\n",
      "756/756 [==============================] - 63s 83ms/sample - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 10/10\n",
      "756/756 [==============================] - 63s 83ms/sample - loss: 0.0020 - val_loss: 0.0019\n"
     ]
    }
   ],
   "source": [
    "# We compile the model:\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "\n",
    "# Finally, we train the model:\n",
    "\n",
    "results = vae.fit(train, train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(test, test)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Variational Autoencoder Encoder Reduced Scissum Dataset Size: :  (756, 250)\n",
      "\n",
      "\n",
      "Variational Autoencoder Scissum Dataset Size: :  (756, 60704)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_encoded = encoder.predict(train)\n",
    "reconstruction = vae.predict(train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Variational Autoencoder Encoder Reduced Scissum Dataset Size: : \", X_encoded.shape)\n",
    "print(\"\\n\")\n",
    "print(\"Variational Autoencoder Scissum Dataset Size: : \", reconstruction.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
